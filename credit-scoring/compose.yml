services:
  mlflow:
    image: credit-ai/mlflow
    build: ./docker/mlflow
    ports:
      - "${MLFLOW_PORT}:${MLFLOW_PORT}"
    env_file:
      - ./.env

    volumes:
      - ./mlflow-artifact:/opt/artifacts
    command: "mlflow server --backend-store-uri ${MLFLOW_BACKEND_URI} --default-artifact-root file:///opt/artifacts --no-serve-artifacts --host ${MLFLOW_HOST} --port ${MLFLOW_PORT}"
  # postgres-db:
  #   image: postgres
  #   environment:
  #     POSTGRES_MULTIPLE_DATABASES: mlflow,credit_scoring
  #     POSTGRES_USER: ${DB_POSTGRES_USER}
  #     POSTGRES_PASSWORD: ${DB_POSTGRES_PASSWORD}
  #     PGPORT: ${DB_POSTGRES_PORT}
  #   env_file:
  #     - ./.env
  #   healthcheck:
  #     test: pg_isready -U ${DB_POSTGRES_USER}
  #     interval: 20s
  #     timeout: 20s
  #     retries: 10
  #     start_period: 20s  
  #   ports:
  #     - "${DB_POSTGRES_PORT}:${DB_POSTGRES_PORT}"
  #   volumes:
  #     - ${DUMMY_DATASET_PATH}:/tmp/
  #     - ./docker/postgres-db/:/docker-entrypoint-initdb.d/
  #     - pg-db-data:/var/lib/postgresql/data
  api:
    image: credit-ai/api
    build: 
      context: ./
      dockerfile: ./docker/api/Dockerfile
    depends_on:
      # postgres-db:
      #   condition: service_healthy
      mlflow:
        condition: service_started
    env_file:
      - ./.env
    environment:
      - PYTHONPATH=/api/src
    ports:
      - "${API_PORT}:${API_PORT}"
    volumes:
      - ./api-logs:/tmp/logs/
      - ./mlflow-artifact:/opt/artifacts
      - ${DUMMY_DATASET_PATH}:/api/dataset/
    entrypoint: ["/bin/sh","-c"]
    command: 
      - |
        alembic upgrade head
        uvicorn src.main:app --host ${API_HOST} --port ${API_PORT} --log-config=./src/config/log-config.yaml
  # redis:
  #   image: redis:7.4.0-alpine
  #   env_file:
  #     - path: ./.env
  #   ports:
  #     - ${REDIS_PORT}:${REDIS_PORT}
  #   expose:
  #     - ${REDIS_PORT}
  #   restart: always
  #   hostname: redis
  #   container_name: redis
  #   volumes:
  #     - redis-data:/data
  #   command: --port ${REDIS_PORT} --requirepass ${REDIS_PASSWORD}
  # celery-beat:
  #   image: credit-api/celery-beat
  #   build: 
  #     context: ./
  #     dockerfile: ./docker/celery/beat/Dockerfile
  #   env_file:
  #     - path: ./.env
  #   volumes:
  #     - celery-beat-data:/app
  #   depends_on:
  #     - redis
  #   command: celery -A src.scheduler.app beat -S redbeat.RedBeatScheduler --loglevel=debug
  # celery-worker:
  #   image: credit-api/celery-worker
  #   build:
  #     context: ./
  #     dockerfile: ./docker/celery/worker/Dockerfile
  #   env_file:
  #     - path: ./.env
  #   volumes:
  #     - celery-worker-data:/app
  #   depends_on:
  #     - redis
  #   command: celery -A src.scheduler.app worker --loglevel=debug
  # celery-flower:
  #   image: credit-api/celery-flower
  #   build:
  #     context: ./
  #     dockerfile: ./docker/celery/flower/Dockerfile
  #   env_file:
  #     - path: ./.env
  #   volumes:
  #     - celery-flower-data:/app
  #   ports:
  #     - ${CELERY_FLOWER_PORT}:${CELERY_FLOWER_PORT}
  #   expose:
  #     - ${CELERY_FLOWER_PORT}
  #   environment:
  #     - CELERY_BROKER_URL=${REDIS_URI}
  #     - CELERY_RESULT_BACKEND=${REDIS_URI}
  #     - C_FORCE_ROOT=true
  #   depends_on:
  #     - redis
  #   command: celery -A src.scheduler.app flower --address=0.0.0.0 --port=${CELERY_FLOWER_PORT} --logLevel=debug

networks:
  default:

volumes:
  # mlflow-artifact:
  pg-db-data:
  # redis-data:
  # celery-beat-data:
  # celery-worker-data:
  # celery-flower-data: